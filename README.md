# Reasoning-papers
Some recent bibliography on reasoning 

**STaR: Bootstrapping Reasoning With Reasoning.**  
Zelikman, Eric, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. March 2022. *arXiv \[Cs\]*.  
<https://arxiv.org/abs/2203.14465>.

**Self-Consistency Improves Chain of Thought Reasoning in Language Models.**  
Wang, Xuezhi, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. March 2022. *arXiv \[Cs\]*.  
<https://arxiv.org/abs/2203.11171>.

**Solving Math Word Problems with Process- and Outcome-Based Feedback.**  
Uesato, Jonathan, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. November 2022. *arXiv \[Cs\]*.  
<https://arxiv.org/abs/2211.14275>.

**Let’s Verify Step by Step.**  
Lightman, Hunter, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. May 2023. *arXiv \[Cs\]*.  
<https://arxiv.org/abs/2305.20050>.

**Reinforced Self-Training (ReST) for Language Modeling.**  
Gulcehre, Caglar, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, et al. August 2023. *arXiv \[Cs\]*.  
<https://arxiv.org/abs/2308.08998>.

**V-STaR: Training Verifiers for Self-Taught Reasoners.**  
Hosseini, Arian, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. February 2024. *arXiv \[Cs\]*.  
<https://arxiv.org/abs/2402.06457>.

**Quiet-: Language Models Can Teach Themselves to Think Before Speaking.**  
Zelikman, Eric, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D. Goodman. March 2024. *arXiv \[Cs\]*.  
<https://arxiv.org/abs/2403.09629>.

**Stream of Search (SoS): Learning to Search in Language.**  
Gandhi, Kanishk, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D. Goodman. April 2024. *arXiv \[Cs\]*.  
<https://arxiv.org/abs/2404.03683>.

**Improve Mathematical Reasoning in Language Models by Automated Process Supervision.**  
Luo, Liangchen, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, et al. June 2024. *arXiv \[Cs\]*.  
<https://arxiv.org/abs/2406.06592>.

**Learn Beyond the Answer: Training Language Models with Reflection for Mathematical Reasoning.**  
Zhang, Zhihan, Tao Ge, Zhenwen Liang, Wenhao Yu, Dian Yu, Mengzhao Jia, Dong Yu, and Meng Jiang. June 2024. *arXiv \[Cs\]*.  
<https://arxiv.org/abs/2406.12050>.

**Lean-STaR: Learning to Interleave Thinking and Proving.**  
Lin, Haohan, Zhiqing Sun, Yiming Yang, and Sean Welleck. July 2024. *arXiv \[Cs\]*.  
<https://arxiv.org/abs/2407.10040>.

**Prover-Verifier Games Improve Legibility of LLM Outputs.**  
Kirchner, Jan Hendrik, Yining Chen, Harri Edwards, Jan Leike, Nat McAleese, and Yuri Burda. July 2024. *arXiv \[Cs\]*.  
<https://arxiv.org/abs/2407.13692>.

**Large Language Monkeys: Scaling Inference Compute with Repeated Sampling.**  
Brown, Bradley, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. July 2024. *arXiv \[Cs\]*.  
<https://arxiv.org/abs/2407.21787>.

**Scaling LLM Test-Time Compute Optimally Can Be More Effective than Scaling Model Parameters.**  
Snell, Charlie, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. August 2024. *arXiv \[Cs\]*.  
<https://arxiv.org/abs/2408.03314>.

**Training Language Models to Self-Correct via Reinforcement Learning.**  
Kumar, Aviral, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D. Co-Reyes, Avi Singh, Kate Baumli, et al. September 2024. *arXiv \[Cs\]*.  
<https://arxiv.org/abs/2409.12917>.

**LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench.**  
Valmeekam, Karthik, Kaya Stechly, and Subbarao Kambhampati. September 2024. *arXiv \[Cs\]*.  
<https://www.arxiv.org/abs/2409.13373>.

**Thinking LLMs: General Instruction Following with Thought Generation.**  
Wu, Tianhao, Janice Lan, Weizhe Yuan, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. October 2024. *arXiv \[Cs\]*.  
<https://arxiv.org/abs/2410.10630>.
